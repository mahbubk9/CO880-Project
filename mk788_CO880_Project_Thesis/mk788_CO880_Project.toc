\contentsline {chapter}{\numberline {1}Introduction}{1}%
\contentsline {chapter}{\numberline {2}Problem Description}{2}%
\contentsline {chapter}{\numberline {3}Literature \& Tech Review}{3}%
\contentsline {section}{\numberline {3.1}Literature Review}{3}%
\contentsline {section}{\numberline {3.2}Tech Review}{3}%
\contentsline {subsection}{\numberline {3.2.1}Reinforcement Learning and Markov Decision Process}{3}%
\contentsline {subsection}{\numberline {3.2.2}Q-Learning}{3}%
\contentsline {subsection}{\numberline {3.2.3}Policy Gradient}{3}%
\contentsline {subsection}{\numberline {3.2.4}Actor-Critic Algorithms}{3}%
\contentsline {subsubsection}{Deep Deterministic Policy Gradient}{3}%
\contentsline {subsubsection}{Advantage Actor Critic}{3}%
\contentsline {chapter}{\numberline {4}Methods}{4}%
\contentsline {subsection}{\numberline {4.0.1}Maximum Entropy RL}{4}%
\contentsline {subsection}{\numberline {4.0.2}Soft Q-Learning and Soft Actor-Critic}{4}%
\contentsline {chapter}{\numberline {5}Implementation}{5}%
\contentsline {subsection}{\numberline {5.0.1}Pseudo Code: Soft Actor-Critic Algorithm}{5}%
\contentsline {chapter}{\numberline {6}Results}{6}%
\contentsline {subsection}{\numberline {6.0.1}Comparison between Deep Deterministic Policy Gradient(DDPG),Advantage Actor Critic(A2C), Soft Actor Critic(SAC)}{6}%
\contentsline {chapter}{\numberline {7}Summary}{7}%
\contentsline {chapter}{\numberline {8}Overview \& Outlook}{8}%
