\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:intro}{{1}{1}}
\citation{Mnih2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Problem Description}{2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:description}{{2}{2}}
\citation{Mnih2015}
\citation{Mnih13}
\citation{ODonoghueMKM16}
\citation{LevineS18:journals/corr/abs-1805-00909}
\citation{Silver14}
\citation{Haaronja17}
\citation{Haaronja18}
\citation{Junhyuk2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Literature \& Tech Review}{3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:review}{{3}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Literature Review}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Tech Review}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Reinforcement Learning and Markov Decision Process}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Q-Learning}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Policy Gradient}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Actor-Critic Algorithms}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Deterministic Policy Gradient}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantage Actor Critic}{3}\protected@file@percent }
\citation{LevineS18:journals/corr/abs-1805-00909}
\citation{Haaronja17}
\citation{Haaronja18}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Methods}{4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:methods}{{4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Maximum Entropy RL}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.2}Soft Q-Learning and Soft Actor-Critic}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:implimentation}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Pseudo Code: Soft Actor-Critic Algorithm}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:results}{{6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}Comparison between Deep Deterministic Policy Gradient(DDPG),Advantage Actor Critic(A2C), Soft Actor Critic(SAC)}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Summary}{7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:summary}{{7}{7}}
\bibstyle{abbrv}
\bibdata{thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Overview \& Outlook}{8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:Evaluation}{{8}{8}}
\bibcite{Haaronja17}{1}
\bibcite{Haaronja18}{2}
\bibcite{LevineS18:journals/corr/abs-1805-00909}{3}
\bibcite{Mnih13}{4}
\bibcite{Mnih2015}{5}
\bibcite{ODonoghueMKM16}{6}
\bibcite{Junhyuk2018}{7}
\bibcite{Silver14}{8}
