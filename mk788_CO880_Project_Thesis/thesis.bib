@article{HausknetchtS15:journals/corr/HausknechtS15,
	author    = {Matthew J. Hausknecht and
	Peter Stone},
	title     = {Deep Recurrent Q-Learning for Partially Observable MDPs},
	journal   = {CoRR},
	volume    = {abs/1507.06527},
	year      = {2015},
	url       = {http://arxiv.org/abs/1507.06527},
	archivePrefix = {arXiv},
	eprint    = {1507.06527},
	timestamp = {Mon, 13 Aug 2018 16:48:38 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HausknechtS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
},
@article{Mnih2015,
  doi = {10.1038/nature14236},
  url = {https://doi.org/10.1038/nature14236},
  year = {2015},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {518},
  number = {7540},
  pages = {529--533},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  title = {Human-level control through deep reinforcement learning},
  journal = {Nature}
},

@article{NarashimanKB15:journals/corr/NarasimhanKB15,
	author    = {Karthik Narasimhan and
	Tejas D. Kulkarni and
	Regina Barzilay},
	title     = {Language Understanding for Text-based Games Using Deep Reinforcement
	Learning},
	journal   = {CoRR},
	volume    = {abs/1506.08941},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.08941},
	archivePrefix = {arXiv},
	eprint    = {1506.08941},
	timestamp = {Mon, 13 Aug 2018 16:48:19 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/NarasimhanKB15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
},
@article{LevineS18:journals/corr/abs-1805-00909,
	author    = {Sergey Levine},
	title     = {Reinforcement Learning and Control as Probabilistic Inference: Tutorial
	and Review},
	journal   = {CoRR},
	volume    = {abs/1805.00909},
	year      = {2018},
	url       = {http://arxiv.org/abs/1805.00909},
	archivePrefix = {arXiv},
	eprint    = {1805.00909},
	timestamp = {Mon, 13 Aug 2018 16:47:19 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1805-00909.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
},

@inproceedings{Silver14,
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	title = {Deterministic Policy Gradient Algorithms},
	year = {2014},
	publisher = {JMLR.org},
	abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
	booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
	pages = {I–387–I–395},
	location = {Beijing, China},
	series = {ICML'14}
},

@InProceedings{Haaronja17, title = {Reinforcement Learning with Deep Energy-Based Policies}, author = {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1352--1361}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf}, url = { http://proceedings.mlr.press/v70/haarnoja17a.html }, abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.} },

@article{Haaronja18,
	author    = {Tuomas Haarnoja and
	Aurick Zhou and
	Kristian Hartikainen and
	George Tucker and
	Sehoon Ha and
	Jie Tan and
	Vikash Kumar and
	Henry Zhu and
	Abhishek Gupta and
	Pieter Abbeel and
	Sergey Levine},
	title     = {Soft Actor-Critic Algorithms and Applications},
	journal   = {CoRR},
	volume    = {abs/1812.05905},
	year      = {2018},
	url       = {http://arxiv.org/abs/1812.05905},
	archivePrefix = {arXiv},
	eprint    = {1812.05905},
	timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1812-05905.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
},

@article{Mnih13,
	author    = {Volodymyr Mnih and
	Koray Kavukcuoglu and
	David Silver and
	Alex Graves and
	Ioannis Antonoglou and
	Daan Wierstra and
	Martin A. Riedmiller},
	title     = {Playing Atari with Deep Reinforcement Learning},
	journal   = {CoRR},
	volume    = {abs/1312.5602},
	year      = {2013},
	url       = {http://arxiv.org/abs/1312.5602},
	archivePrefix = {arXiv},
	eprint    = {1312.5602},
	timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
},

@article{ODonoghueMKM16,
	author    = {Brendan O'Donoghue and
	R{\'{e}}mi Munos and
	Koray Kavukcuoglu and
	Volodymyr Mnih},
	title     = {{PGQ:} Combining policy gradient and Q-learning},
	journal   = {CoRR},
	volume    = {abs/1611.01626},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.01626},
	archivePrefix = {arXiv},
	eprint    = {1611.01626},
	timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ODonoghueMKM16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
},

@article{Junhyuk2018,
	author    = {Junhyuk Oh and
	Yijie Guo and
	Satinder Singh and
	Honglak Lee},
	title     = {Self-Imitation Learning},
	journal   = {CoRR},
	volume    = {abs/1806.05635},
	year      = {2018},
	url       = {http://arxiv.org/abs/1806.05635},
	archivePrefix = {arXiv},
	eprint    = {1806.05635},
	timestamp = {Mon, 13 Aug 2018 16:48:33 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1806-05635.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}